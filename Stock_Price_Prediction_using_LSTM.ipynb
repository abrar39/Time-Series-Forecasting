{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjQxH/t4P0G/rywY5sM3AB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrar39/Time-Series-Forecasting/blob/master/Stock_Price_Prediction_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aIFKQ6Lr7Xs",
        "outputId": "bc5a10d8-52e3-4363-c188-739931a6ec28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psx-data-reader in /usr/local/lib/python3.10/dist-packages (0.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from psx-data-reader) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from psx-data-reader) (4.66.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from psx-data-reader) (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from psx-data-reader) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->psx-data-reader) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->psx-data-reader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->psx-data-reader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->psx-data-reader) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->psx-data-reader) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->psx-data-reader) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->psx-data-reader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->psx-data-reader) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->psx-data-reader) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->psx-data-reader) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install psx-data-reader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psx import stocks, tickers\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "lpL274k2sh9U"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "wandb.login(key=userdata.get('wandb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf3LhwS61DbN",
        "outputId": "2144fde2-5307-4a4f-8c3d-e08201824a36"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"time-series-forecasting\", name=\"exp-001\", config={\n",
        "    \"hidden_size\": 10,\n",
        "    \"num_layers\": 1,\n",
        "    \"dropout\": 0.5,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\" : 500\n",
        "})\n",
        "\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "aCb8NUu72HzC",
        "outputId": "bc1c702a-e158-4fd5-a9a8-759a7c4a05f2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241215_151943-gid0gz4s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abrar39/time-series-forecasting/runs/gid0gz4s' target=\"_blank\">exp-001</a></strong> to <a href='https://wandb.ai/abrar39/time-series-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abrar39/time-series-forecasting' target=\"_blank\">https://wandb.ai/abrar39/time-series-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abrar39/time-series-forecasting/runs/gid0gz4s' target=\"_blank\">https://wandb.ai/abrar39/time-series-forecasting/runs/gid0gz4s</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Companies to scrape\n",
        "companies = ['EFERT', 'ILP', 'PABC']\n",
        "data = stocks(companies, start=datetime.date(2020, 1, 1), end=datetime.date.today())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrTJRyvIsKhp",
        "outputId": "60fc59b5-c678-4aaf-9e1a-193f8fd00c46"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading EFERT's Data: 100%|██████████| 61/61 [00:08<00:00,  7.02it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/psx/web.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  data = pd.concat(data)\n",
            "Downloading ILP's Data: 100%|██████████| 61/61 [00:08<00:00,  7.39it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/psx/web.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  data = pd.concat(data)\n",
            "Downloading PABC's Data: 100%|██████████| 61/61 [00:07<00:00,  7.96it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/psx/web.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  data = pd.concat(data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def company_eda(company_name : str, print_head : bool = True, plot_historical_values : bool = True):\n",
        "  # Print the head of the company data\n",
        "  if print_head:\n",
        "    print(\"Printing Head...\")\n",
        "    print(data.loc[company_name].head())\n",
        "\n",
        "  if plot_historical_values:\n",
        "    # Plot the historical values of the company\n",
        "    data.loc[company_name]['Close'].plot()\n",
        "\n",
        "  # Print the summary statistics of the company\n",
        "  print(\"Printing Summary Statistics...\")\n",
        "  print(data.loc[company_name].describe())"
      ],
      "metadata": {
        "id": "4H0_GFrd5p8M"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_eda('EFERT')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "G9vBdSXK6mTo",
        "outputId": "27b384b1-eaa0-48d4-a185-c4776090b399"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing Head...\n",
            "             Open   High    Low  Close     Volume\n",
            "Date                                             \n",
            "2020-01-01  73.44  74.40  73.29  73.86  1039500.0\n",
            "2020-01-02  74.15  74.57  73.60  74.01  3035000.0\n",
            "2020-01-03  74.20  74.29  73.19  73.57  2232000.0\n",
            "2020-01-06  72.98  73.40  71.00  72.64  1577500.0\n",
            "2020-01-07  72.84  74.19  72.00  73.57  2328500.0\n",
            "Printing Summary Statistics...\n",
            "              Open         High          Low        Close        Volume\n",
            "count  1227.000000  1227.000000  1227.000000  1227.000000  1.227000e+03\n",
            "mean     92.627425    93.667873    91.670098    92.680024  1.854067e+06\n",
            "std      37.570737    38.229402    36.988647    37.645724  1.820383e+06\n",
            "min      50.010000    52.950000    48.990000    50.720000  1.375870e+05\n",
            "25%      67.510000    68.000000    66.945000    67.550000  7.889565e+05\n",
            "50%      80.300000    81.000000    79.800000    80.410000  1.334509e+06\n",
            "75%      90.275000    91.440000    89.505000    90.570000  2.204892e+06\n",
            "max     213.000000   215.000000   208.100000   210.280000  1.850787e+07\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_windowed_data(company_data, window_size):\n",
        "  periods = np.arange(1, window_size+1)  # T-1 to T-n where n = window size\n",
        "  company_data = company_data.assign(**{f\"Close T-{period}\": company_data['Close'].shift(period) for period in periods})\n",
        "\n",
        "  return company_data.dropna()"
      ],
      "metadata": {
        "id": "oixGT3-n64ox"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efert_data = data.loc['EFERT'].copy()\n",
        "efert_data = generate_windowed_data(efert_data, 10)\n",
        "efert_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "j5voujRB7sLr",
        "outputId": "ac7ecfa0-b85c-429e-b2cb-83ecac3b7d68"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Open   High    Low  Close      Volume  Close T-1  Close T-2  \\\n",
              "Date                                                                      \n",
              "2020-01-15  75.5  75.75  74.86  75.07    594500.0      75.56      75.05   \n",
              "2020-01-16  75.2  75.30  74.61  75.16    588500.0      75.07      75.56   \n",
              "2020-01-17  75.0  75.59  74.75  75.30   1323000.0      75.16      75.07   \n",
              "2020-01-20  75.1  75.15  71.16  71.17  15269000.0      75.30      75.16   \n",
              "2020-01-21  69.0  70.14  68.20  68.49   6675000.0      71.17      75.30   \n",
              "\n",
              "            Close T-3  Close T-4  Close T-5  Close T-6  Close T-7  Close T-8  \\\n",
              "Date                                                                           \n",
              "2020-01-15      74.94      74.83      72.84      73.57      72.64      73.57   \n",
              "2020-01-16      75.05      74.94      74.83      72.84      73.57      72.64   \n",
              "2020-01-17      75.56      75.05      74.94      74.83      72.84      73.57   \n",
              "2020-01-20      75.07      75.56      75.05      74.94      74.83      72.84   \n",
              "2020-01-21      75.16      75.07      75.56      75.05      74.94      74.83   \n",
              "\n",
              "            Close T-9  Close T-10  \n",
              "Date                               \n",
              "2020-01-15      74.01       73.86  \n",
              "2020-01-16      73.57       74.01  \n",
              "2020-01-17      72.64       73.57  \n",
              "2020-01-20      73.57       72.64  \n",
              "2020-01-21      72.84       73.57  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d6b22fc-ee4a-41a7-86de-e555aae235e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Close T-1</th>\n",
              "      <th>Close T-2</th>\n",
              "      <th>Close T-3</th>\n",
              "      <th>Close T-4</th>\n",
              "      <th>Close T-5</th>\n",
              "      <th>Close T-6</th>\n",
              "      <th>Close T-7</th>\n",
              "      <th>Close T-8</th>\n",
              "      <th>Close T-9</th>\n",
              "      <th>Close T-10</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-15</th>\n",
              "      <td>75.5</td>\n",
              "      <td>75.75</td>\n",
              "      <td>74.86</td>\n",
              "      <td>75.07</td>\n",
              "      <td>594500.0</td>\n",
              "      <td>75.56</td>\n",
              "      <td>75.05</td>\n",
              "      <td>74.94</td>\n",
              "      <td>74.83</td>\n",
              "      <td>72.84</td>\n",
              "      <td>73.57</td>\n",
              "      <td>72.64</td>\n",
              "      <td>73.57</td>\n",
              "      <td>74.01</td>\n",
              "      <td>73.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-16</th>\n",
              "      <td>75.2</td>\n",
              "      <td>75.30</td>\n",
              "      <td>74.61</td>\n",
              "      <td>75.16</td>\n",
              "      <td>588500.0</td>\n",
              "      <td>75.07</td>\n",
              "      <td>75.56</td>\n",
              "      <td>75.05</td>\n",
              "      <td>74.94</td>\n",
              "      <td>74.83</td>\n",
              "      <td>72.84</td>\n",
              "      <td>73.57</td>\n",
              "      <td>72.64</td>\n",
              "      <td>73.57</td>\n",
              "      <td>74.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-17</th>\n",
              "      <td>75.0</td>\n",
              "      <td>75.59</td>\n",
              "      <td>74.75</td>\n",
              "      <td>75.30</td>\n",
              "      <td>1323000.0</td>\n",
              "      <td>75.16</td>\n",
              "      <td>75.07</td>\n",
              "      <td>75.56</td>\n",
              "      <td>75.05</td>\n",
              "      <td>74.94</td>\n",
              "      <td>74.83</td>\n",
              "      <td>72.84</td>\n",
              "      <td>73.57</td>\n",
              "      <td>72.64</td>\n",
              "      <td>73.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-20</th>\n",
              "      <td>75.1</td>\n",
              "      <td>75.15</td>\n",
              "      <td>71.16</td>\n",
              "      <td>71.17</td>\n",
              "      <td>15269000.0</td>\n",
              "      <td>75.30</td>\n",
              "      <td>75.16</td>\n",
              "      <td>75.07</td>\n",
              "      <td>75.56</td>\n",
              "      <td>75.05</td>\n",
              "      <td>74.94</td>\n",
              "      <td>74.83</td>\n",
              "      <td>72.84</td>\n",
              "      <td>73.57</td>\n",
              "      <td>72.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-21</th>\n",
              "      <td>69.0</td>\n",
              "      <td>70.14</td>\n",
              "      <td>68.20</td>\n",
              "      <td>68.49</td>\n",
              "      <td>6675000.0</td>\n",
              "      <td>71.17</td>\n",
              "      <td>75.30</td>\n",
              "      <td>75.16</td>\n",
              "      <td>75.07</td>\n",
              "      <td>75.56</td>\n",
              "      <td>75.05</td>\n",
              "      <td>74.94</td>\n",
              "      <td>74.83</td>\n",
              "      <td>72.84</td>\n",
              "      <td>73.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d6b22fc-ee4a-41a7-86de-e555aae235e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d6b22fc-ee4a-41a7-86de-e555aae235e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d6b22fc-ee4a-41a7-86de-e555aae235e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-61ebde3d-561d-49ae-a365-1e2b51f68a19\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61ebde3d-561d-49ae-a365-1e2b51f68a19')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-61ebde3d-561d-49ae-a365-1e2b51f68a19 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "efert_data",
              "summary": "{\n  \"name\": \"efert_data\",\n  \"rows\": 1217,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-15 00:00:00\",\n        \"max\": \"2024-12-13 00:00:00\",\n        \"num_unique_values\": 1217,\n        \"samples\": [\n          \"2022-03-17 00:00:00\",\n          \"2021-01-27 00:00:00\",\n          \"2020-03-17 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.686372337150914,\n        \"min\": 50.01,\n        \"max\": 213.0,\n        \"num_unique_values\": 896,\n        \"samples\": [\n          123.0,\n          90.25,\n          81.65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.34693998283495,\n        \"min\": 52.95,\n        \"max\": 215.0,\n        \"num_unique_values\": 882,\n        \"samples\": [\n          52.95,\n          75.25,\n          60.95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.102385541690204,\n        \"min\": 48.99,\n        \"max\": 208.1,\n        \"num_unique_values\": 887,\n        \"samples\": [\n          68.1,\n          121.1,\n          78.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.762173659556055,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1081,\n        \"samples\": [\n          73.42,\n          91.56,\n          85.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1826762.1408785724,\n        \"min\": 137587.0,\n        \"max\": 18507868.0,\n        \"num_unique_values\": 1217,\n        \"samples\": [\n          530891.0,\n          778941.0,\n          1588500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.628919066628114,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1081,\n        \"samples\": [\n          73.86,\n          91.16,\n          86.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.48313891144871,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1081,\n        \"samples\": [\n          73.82,\n          90.56,\n          86.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.352820205478025,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1081,\n        \"samples\": [\n          73.08,\n          89.08,\n          85.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.21968335280094,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1080,\n        \"samples\": [\n          72.68,\n          89.08,\n          85.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.0704900059401,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1079,\n        \"samples\": [\n          72.68,\n          89.08,\n          85.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.922967497808905,\n        \"min\": 50.72,\n        \"max\": 210.28,\n        \"num_unique_values\": 1079,\n        \"samples\": [\n          70.65,\n          89.51,\n          85.58\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.77141464577908,\n        \"min\": 50.72,\n        \"max\": 208.75,\n        \"num_unique_values\": 1079,\n        \"samples\": [\n          74.6,\n          88.89,\n          87.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.631723357850284,\n        \"min\": 50.72,\n        \"max\": 208.75,\n        \"num_unique_values\": 1078,\n        \"samples\": [\n          74.6,\n          88.89,\n          100.41\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.49015330555313,\n        \"min\": 50.72,\n        \"max\": 208.75,\n        \"num_unique_values\": 1078,\n        \"samples\": [\n          73.78,\n          87.99,\n          100.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close T-10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.35766141425901,\n        \"min\": 50.72,\n        \"max\": 208.75,\n        \"num_unique_values\": 1077,\n        \"samples\": [\n          75.13,\n          87.99,\n          160.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the data ready. We can begin creation of our model for prediction."
      ],
      "metadata": {
        "id": "R8cjuYB7-6SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the X, and y arrays containing the features, and targets respectively.\n",
        "def create_x_y(df, window_size):\n",
        "  # select X columns\n",
        "  X_columns = [f\"Close T-{period}\" for period in np.arange(1, window_size+1)]\n",
        "  y_column = [\"Close\"]\n",
        "\n",
        "  X = df[X_columns].values # we need numpy arrays for training\n",
        "  y = df[y_column].values\n",
        "\n",
        "  # Reshape the X matrix as required by the training algorithm\n",
        "  X = X.reshape(X.shape[0], X.shape[1], 1) # [num_samples, sequence_length, num_features]\n",
        "  #X = X.reshape(X.shape[0], 5, 1)  # [num_samples, sequence_length, num_features]\n",
        "  return X.astype(np.float32), y.astype(np.float32)"
      ],
      "metadata": {
        "id": "kK35Yp4U85VT"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = create_x_y(efert_data, 10)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDa_YyFwBwHr",
        "outputId": "c6baa852-bf4e-4935-aa91-cf8614f0b321"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1217, 10, 1), (1217, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train, test, and validation sets\n",
        "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, shuffle=False)"
      ],
      "metadata": {
        "id": "7KxANGckCYwQ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the data into tensors for use in pytorch\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "X_val = torch.from_numpy(X_val).float()\n",
        "y_val = torch.from_numpy(y_val).float()"
      ],
      "metadata": {
        "id": "uJgrQhP4L7Xf"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beWYC9P_MWMf",
        "outputId": "4ed93e86-b093-4860-bd55-d2ed47f82d10"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([973, 10, 1]), torch.Size([973, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Model\n",
        "I shall use pytorch sequential model to create the LSTM model."
      ],
      "metadata": {
        "id": "InLihugRDXxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the device to use for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "k1OZmUkgET8S"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the LSTM-based model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        #self.input_layer = nn.Linear(5, 5)  # Input transformation layer (if needed)\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=config.hidden_size, num_layers=config.num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.input_layer(x)  # Transform the input (optional, depending on data)\n",
        "        lstm_out, (hidden_state, cell_state) = self.lstm(x)  # Apply LSTM\n",
        "        x = self.fc1(lstm_out[:, -1, :])  # Use the last hidden state\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate and print the model\n",
        "model = LSTMModel()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg3OqfhCErED",
        "outputId": "1c27c94c-fe3f-4c06-fc71-6a92e9f424d6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel(\n",
            "  (lstm): LSTM(1, 10, batch_first=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, loss, optimizer, batch_size, device, epochs=100, patience=5):\n",
        "\n",
        "  best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
        "  epochs_since_improvement = 0  # Counter for the number of epochs without improvement\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)  # Pass inputs through the model\n",
        "        loss = criterion(outputs, targets)  # Compute the loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Backpropagation\n",
        "        optimizer.step()       # Update the model parameters\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    #print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "  # Compute validation loss at the end of each epoch\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss / len(train_loader):.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Check if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # Update the best validation loss\n",
        "        epochs_since_improvement = 0  # Reset the counter if improvement occurs\n",
        "    else:\n",
        "        epochs_since_improvement += 1  # Increment the counter if no improvement\n",
        "\n",
        "    # Log the results to WANDB\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": epoch_loss / len(train_loader),\n",
        "        \"val_loss\": val_loss.item()\n",
        "    })\n",
        "\n",
        "    # Early stopping condition\n",
        "    if epochs_since_improvement >= patience:\n",
        "        print(f\"Early stopping after {epoch + 1} epochs\")\n",
        "        break  # Stop training if no improvement after `patience` epochs"
      ],
      "metadata": {
        "id": "YXVyPVDDTL5Q"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()  # the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate) # the optimizer\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = config.batch_size\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Epochs\n",
        "num_epochs = config.epochs\n",
        "patience=5 # early stopping criteria\n",
        "\n",
        "train_model(model, train_loader, loss=criterion, optimizer=optimizer, batch_size=batch_size, device=device, epochs=num_epochs, patience=patience)\n",
        "\n",
        "# Save and log the model\n",
        "torch.save(model.state_dict(), \"model-exp-001.pth\")\n",
        "wandb.save(\"model-exp-001.pth\")\n",
        "\n",
        "# End the W&B run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5Y-9pzuT0nU",
        "outputId": "ceee78f7-f171-4829-aa65-7d959f431969"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Train Loss: 5878.8886, Val Loss: 20051.5371\n",
            "Epoch 2/500, Train Loss: 5827.1881, Val Loss: 19963.2910\n",
            "Epoch 3/500, Train Loss: 5767.7298, Val Loss: 19813.2148\n",
            "Epoch 4/500, Train Loss: 5668.4941, Val Loss: 19555.4004\n",
            "Epoch 5/500, Train Loss: 5487.7836, Val Loss: 19211.7754\n",
            "Epoch 6/500, Train Loss: 5275.2388, Val Loss: 18755.7480\n",
            "Epoch 7/500, Train Loss: 5077.6141, Val Loss: 18393.5430\n",
            "Epoch 8/500, Train Loss: 4886.2783, Val Loss: 18010.8496\n",
            "Epoch 9/500, Train Loss: 4683.3734, Val Loss: 17607.5547\n",
            "Epoch 10/500, Train Loss: 4487.1120, Val Loss: 17183.6016\n",
            "Epoch 11/500, Train Loss: 4262.0314, Val Loss: 16739.3340\n",
            "Epoch 12/500, Train Loss: 4031.7688, Val Loss: 16277.5469\n",
            "Epoch 13/500, Train Loss: 3813.4374, Val Loss: 15801.9414\n",
            "Epoch 14/500, Train Loss: 3583.5708, Val Loss: 15312.2598\n",
            "Epoch 15/500, Train Loss: 3347.5848, Val Loss: 14814.2705\n",
            "Epoch 16/500, Train Loss: 3129.4557, Val Loss: 14307.4072\n",
            "Epoch 17/500, Train Loss: 2884.5775, Val Loss: 13796.4023\n",
            "Epoch 18/500, Train Loss: 2653.4239, Val Loss: 13285.8574\n",
            "Epoch 19/500, Train Loss: 2427.0733, Val Loss: 12774.5469\n",
            "Epoch 20/500, Train Loss: 2225.8453, Val Loss: 12268.1426\n",
            "Epoch 21/500, Train Loss: 2018.0710, Val Loss: 11767.7461\n",
            "Epoch 22/500, Train Loss: 1819.5022, Val Loss: 11276.2578\n",
            "Epoch 23/500, Train Loss: 1634.3003, Val Loss: 10794.8555\n",
            "Epoch 24/500, Train Loss: 1459.8466, Val Loss: 10325.4072\n",
            "Epoch 25/500, Train Loss: 1302.4654, Val Loss: 9875.3721\n",
            "Epoch 26/500, Train Loss: 1144.7730, Val Loss: 9436.8262\n",
            "Epoch 27/500, Train Loss: 1005.4431, Val Loss: 9017.0898\n",
            "Epoch 28/500, Train Loss: 885.4508, Val Loss: 8622.0020\n",
            "Epoch 29/500, Train Loss: 774.6092, Val Loss: 8243.7822\n",
            "Epoch 30/500, Train Loss: 667.6850, Val Loss: 7883.7720\n",
            "Epoch 31/500, Train Loss: 579.1989, Val Loss: 7550.1050\n",
            "Epoch 32/500, Train Loss: 505.8611, Val Loss: 7237.2080\n",
            "Epoch 33/500, Train Loss: 436.4305, Val Loss: 6942.0161\n",
            "Epoch 34/500, Train Loss: 373.5107, Val Loss: 6673.2939\n",
            "Epoch 35/500, Train Loss: 328.6621, Val Loss: 6428.3125\n",
            "Epoch 36/500, Train Loss: 284.2330, Val Loss: 6201.0146\n",
            "Epoch 37/500, Train Loss: 253.2690, Val Loss: 5995.3188\n",
            "Epoch 38/500, Train Loss: 222.0935, Val Loss: 5809.4287\n",
            "Epoch 39/500, Train Loss: 199.3768, Val Loss: 5638.3394\n",
            "Epoch 40/500, Train Loss: 183.2499, Val Loss: 5489.2769\n",
            "Epoch 41/500, Train Loss: 167.0591, Val Loss: 5351.9756\n",
            "Epoch 42/500, Train Loss: 157.5737, Val Loss: 5238.8350\n",
            "Epoch 43/500, Train Loss: 146.9026, Val Loss: 5130.1035\n",
            "Epoch 44/500, Train Loss: 140.2069, Val Loss: 5035.0737\n",
            "Epoch 45/500, Train Loss: 135.1597, Val Loss: 4954.5283\n",
            "Epoch 46/500, Train Loss: 133.6350, Val Loss: 4879.2441\n",
            "Epoch 47/500, Train Loss: 128.0432, Val Loss: 4818.3623\n",
            "Epoch 48/500, Train Loss: 126.0436, Val Loss: 4773.4580\n",
            "Epoch 49/500, Train Loss: 123.3366, Val Loss: 4723.6211\n",
            "Epoch 50/500, Train Loss: 122.0299, Val Loss: 4677.2617\n",
            "Epoch 51/500, Train Loss: 118.0330, Val Loss: 4627.3735\n",
            "Epoch 52/500, Train Loss: 115.4761, Val Loss: 4580.6660\n",
            "Epoch 53/500, Train Loss: 104.8873, Val Loss: 4511.8472\n",
            "Epoch 54/500, Train Loss: 98.4284, Val Loss: 4442.3252\n",
            "Epoch 55/500, Train Loss: 93.9919, Val Loss: 4375.3984\n",
            "Epoch 56/500, Train Loss: 89.4019, Val Loss: 4312.6670\n",
            "Epoch 57/500, Train Loss: 85.5212, Val Loss: 4242.0288\n",
            "Epoch 58/500, Train Loss: 82.4836, Val Loss: 4181.2056\n",
            "Epoch 59/500, Train Loss: 77.0693, Val Loss: 4125.2510\n",
            "Epoch 60/500, Train Loss: 73.8639, Val Loss: 4065.3667\n",
            "Epoch 61/500, Train Loss: 70.4200, Val Loss: 4009.2395\n",
            "Epoch 62/500, Train Loss: 67.3376, Val Loss: 3956.9771\n",
            "Epoch 63/500, Train Loss: 65.1639, Val Loss: 3905.7632\n",
            "Epoch 64/500, Train Loss: 60.7343, Val Loss: 3852.0884\n",
            "Epoch 65/500, Train Loss: 58.7823, Val Loss: 3809.8589\n",
            "Epoch 66/500, Train Loss: 55.4802, Val Loss: 3753.4050\n",
            "Epoch 67/500, Train Loss: 52.4461, Val Loss: 3707.6082\n",
            "Epoch 68/500, Train Loss: 50.1426, Val Loss: 3660.0876\n",
            "Epoch 69/500, Train Loss: 47.8616, Val Loss: 3617.5542\n",
            "Epoch 70/500, Train Loss: 45.6614, Val Loss: 3568.5913\n",
            "Epoch 71/500, Train Loss: 43.7722, Val Loss: 3527.8252\n",
            "Epoch 72/500, Train Loss: 41.5569, Val Loss: 3488.7695\n",
            "Epoch 73/500, Train Loss: 39.2734, Val Loss: 3445.4077\n",
            "Epoch 74/500, Train Loss: 37.8089, Val Loss: 3403.2717\n",
            "Epoch 75/500, Train Loss: 35.9032, Val Loss: 3366.5193\n",
            "Epoch 76/500, Train Loss: 34.0141, Val Loss: 3328.4500\n",
            "Epoch 77/500, Train Loss: 32.2571, Val Loss: 3291.2327\n",
            "Epoch 78/500, Train Loss: 30.9939, Val Loss: 3256.6506\n",
            "Epoch 79/500, Train Loss: 29.3585, Val Loss: 3219.9875\n",
            "Epoch 80/500, Train Loss: 28.4546, Val Loss: 3187.1389\n",
            "Epoch 81/500, Train Loss: 26.5723, Val Loss: 3153.8059\n",
            "Epoch 82/500, Train Loss: 25.9889, Val Loss: 3125.8704\n",
            "Epoch 83/500, Train Loss: 24.2603, Val Loss: 3093.2678\n",
            "Epoch 84/500, Train Loss: 23.3987, Val Loss: 3062.4165\n",
            "Epoch 85/500, Train Loss: 22.4080, Val Loss: 3036.0508\n",
            "Epoch 86/500, Train Loss: 21.5812, Val Loss: 3010.0000\n",
            "Epoch 87/500, Train Loss: 21.2901, Val Loss: 2982.1001\n",
            "Epoch 88/500, Train Loss: 20.8314, Val Loss: 2958.8577\n",
            "Epoch 89/500, Train Loss: 19.5944, Val Loss: 2929.7927\n",
            "Epoch 90/500, Train Loss: 19.0351, Val Loss: 2908.1599\n",
            "Epoch 91/500, Train Loss: 19.7335, Val Loss: 2886.7461\n",
            "Epoch 92/500, Train Loss: 18.0348, Val Loss: 2861.5198\n",
            "Epoch 93/500, Train Loss: 17.8698, Val Loss: 2838.5112\n",
            "Epoch 94/500, Train Loss: 17.5656, Val Loss: 2820.0691\n",
            "Epoch 95/500, Train Loss: 16.9861, Val Loss: 2798.7698\n",
            "Epoch 96/500, Train Loss: 16.5016, Val Loss: 2781.9070\n",
            "Epoch 97/500, Train Loss: 17.5706, Val Loss: 2760.9658\n",
            "Epoch 98/500, Train Loss: 16.1724, Val Loss: 2742.2698\n",
            "Epoch 99/500, Train Loss: 16.0083, Val Loss: 2722.2048\n",
            "Epoch 100/500, Train Loss: 15.8098, Val Loss: 2707.3909\n",
            "Epoch 101/500, Train Loss: 15.7524, Val Loss: 2693.4561\n",
            "Epoch 102/500, Train Loss: 15.7409, Val Loss: 2675.3794\n",
            "Epoch 103/500, Train Loss: 16.2226, Val Loss: 2661.6489\n",
            "Epoch 104/500, Train Loss: 15.3536, Val Loss: 2644.1545\n",
            "Epoch 105/500, Train Loss: 15.4308, Val Loss: 2630.2642\n",
            "Epoch 106/500, Train Loss: 14.9671, Val Loss: 2616.6174\n",
            "Epoch 107/500, Train Loss: 14.9141, Val Loss: 2601.2427\n",
            "Epoch 108/500, Train Loss: 14.6810, Val Loss: 2587.0710\n",
            "Epoch 109/500, Train Loss: 14.6282, Val Loss: 2575.2339\n",
            "Epoch 110/500, Train Loss: 14.5914, Val Loss: 2561.7224\n",
            "Epoch 111/500, Train Loss: 14.8199, Val Loss: 2552.3162\n",
            "Epoch 112/500, Train Loss: 14.6042, Val Loss: 2538.3647\n",
            "Epoch 113/500, Train Loss: 14.7771, Val Loss: 2528.5166\n",
            "Epoch 114/500, Train Loss: 14.5710, Val Loss: 2516.3547\n",
            "Epoch 115/500, Train Loss: 14.2606, Val Loss: 2506.0791\n",
            "Epoch 116/500, Train Loss: 14.0773, Val Loss: 2495.0247\n",
            "Epoch 117/500, Train Loss: 14.4831, Val Loss: 2486.6511\n",
            "Epoch 118/500, Train Loss: 14.1353, Val Loss: 2472.9946\n",
            "Epoch 119/500, Train Loss: 14.2909, Val Loss: 2466.4414\n",
            "Epoch 120/500, Train Loss: 14.0144, Val Loss: 2453.8298\n",
            "Epoch 121/500, Train Loss: 13.9370, Val Loss: 2447.3171\n",
            "Epoch 122/500, Train Loss: 13.8172, Val Loss: 2439.0950\n",
            "Epoch 123/500, Train Loss: 13.8100, Val Loss: 2425.7117\n",
            "Epoch 124/500, Train Loss: 14.1171, Val Loss: 2416.8218\n",
            "Epoch 125/500, Train Loss: 14.3284, Val Loss: 2410.3694\n",
            "Epoch 126/500, Train Loss: 14.5488, Val Loss: 2404.7205\n",
            "Epoch 127/500, Train Loss: 14.0068, Val Loss: 2395.0056\n",
            "Epoch 128/500, Train Loss: 14.1172, Val Loss: 2383.8467\n",
            "Epoch 129/500, Train Loss: 13.7582, Val Loss: 2376.5405\n",
            "Epoch 130/500, Train Loss: 14.1186, Val Loss: 2368.6606\n",
            "Epoch 131/500, Train Loss: 13.7590, Val Loss: 2358.5056\n",
            "Epoch 132/500, Train Loss: 13.5713, Val Loss: 2352.3916\n",
            "Epoch 133/500, Train Loss: 13.6617, Val Loss: 2347.0986\n",
            "Epoch 134/500, Train Loss: 13.6330, Val Loss: 2334.5625\n",
            "Epoch 135/500, Train Loss: 13.8676, Val Loss: 2329.2051\n",
            "Epoch 136/500, Train Loss: 14.6887, Val Loss: 2326.6025\n",
            "Epoch 137/500, Train Loss: 13.5584, Val Loss: 2313.1626\n",
            "Epoch 138/500, Train Loss: 13.6957, Val Loss: 2308.8875\n",
            "Epoch 139/500, Train Loss: 13.4855, Val Loss: 2300.6995\n",
            "Epoch 140/500, Train Loss: 13.3725, Val Loss: 2293.4160\n",
            "Epoch 141/500, Train Loss: 13.5678, Val Loss: 2290.6733\n",
            "Epoch 142/500, Train Loss: 13.4668, Val Loss: 2280.1287\n",
            "Epoch 143/500, Train Loss: 13.7055, Val Loss: 2274.4985\n",
            "Epoch 144/500, Train Loss: 13.6229, Val Loss: 2271.0281\n",
            "Epoch 145/500, Train Loss: 13.9920, Val Loss: 2272.5400\n",
            "Epoch 146/500, Train Loss: 13.8044, Val Loss: 2258.0835\n",
            "Epoch 147/500, Train Loss: 13.4973, Val Loss: 2257.5059\n",
            "Epoch 148/500, Train Loss: 13.4302, Val Loss: 2251.1736\n",
            "Epoch 149/500, Train Loss: 13.7555, Val Loss: 2239.9756\n",
            "Epoch 150/500, Train Loss: 14.2226, Val Loss: 2241.3872\n",
            "Epoch 151/500, Train Loss: 13.7009, Val Loss: 2229.2173\n",
            "Epoch 152/500, Train Loss: 13.3065, Val Loss: 2227.1626\n",
            "Epoch 153/500, Train Loss: 13.4293, Val Loss: 2218.5806\n",
            "Epoch 154/500, Train Loss: 13.1138, Val Loss: 2216.1851\n",
            "Epoch 155/500, Train Loss: 13.2766, Val Loss: 2204.0068\n",
            "Epoch 156/500, Train Loss: 13.7233, Val Loss: 2200.5886\n",
            "Epoch 157/500, Train Loss: 13.5195, Val Loss: 2197.5640\n",
            "Epoch 158/500, Train Loss: 13.3431, Val Loss: 2192.9341\n",
            "Epoch 159/500, Train Loss: 13.2814, Val Loss: 2188.4126\n",
            "Epoch 160/500, Train Loss: 13.6018, Val Loss: 2186.9053\n",
            "Epoch 161/500, Train Loss: 13.3883, Val Loss: 2177.0659\n",
            "Epoch 162/500, Train Loss: 13.2108, Val Loss: 2172.0696\n",
            "Epoch 163/500, Train Loss: 13.6346, Val Loss: 2163.7788\n",
            "Epoch 164/500, Train Loss: 13.7265, Val Loss: 2158.6614\n",
            "Epoch 165/500, Train Loss: 13.6179, Val Loss: 2158.0288\n",
            "Epoch 166/500, Train Loss: 13.6518, Val Loss: 2149.4810\n",
            "Epoch 167/500, Train Loss: 13.4448, Val Loss: 2144.4385\n",
            "Epoch 168/500, Train Loss: 13.5053, Val Loss: 2149.6516\n",
            "Epoch 169/500, Train Loss: 13.3011, Val Loss: 2138.7241\n",
            "Epoch 170/500, Train Loss: 13.3996, Val Loss: 2126.0481\n",
            "Epoch 171/500, Train Loss: 13.8286, Val Loss: 2130.5210\n",
            "Epoch 172/500, Train Loss: 13.1656, Val Loss: 2121.5361\n",
            "Epoch 173/500, Train Loss: 13.8854, Val Loss: 2116.8608\n",
            "Epoch 174/500, Train Loss: 13.1211, Val Loss: 2106.8069\n",
            "Epoch 175/500, Train Loss: 13.1116, Val Loss: 2109.1057\n",
            "Epoch 176/500, Train Loss: 13.1590, Val Loss: 2097.2781\n",
            "Epoch 177/500, Train Loss: 13.1703, Val Loss: 2097.2866\n",
            "Epoch 178/500, Train Loss: 13.2449, Val Loss: 2095.7537\n",
            "Epoch 179/500, Train Loss: 13.0065, Val Loss: 2086.7808\n",
            "Epoch 180/500, Train Loss: 13.2641, Val Loss: 2086.6846\n",
            "Epoch 181/500, Train Loss: 13.6787, Val Loss: 2082.1990\n",
            "Epoch 182/500, Train Loss: 13.0817, Val Loss: 2067.8440\n",
            "Epoch 183/500, Train Loss: 12.9861, Val Loss: 2070.4990\n",
            "Epoch 184/500, Train Loss: 13.0406, Val Loss: 2067.5176\n",
            "Epoch 185/500, Train Loss: 13.0159, Val Loss: 2054.7231\n",
            "Epoch 186/500, Train Loss: 13.2680, Val Loss: 2046.4856\n",
            "Epoch 187/500, Train Loss: 13.5216, Val Loss: 2048.4561\n",
            "Epoch 188/500, Train Loss: 13.0784, Val Loss: 2041.3147\n",
            "Epoch 189/500, Train Loss: 13.0446, Val Loss: 2041.3279\n",
            "Epoch 190/500, Train Loss: 12.9581, Val Loss: 2030.2273\n",
            "Epoch 191/500, Train Loss: 12.8833, Val Loss: 2029.7631\n",
            "Epoch 192/500, Train Loss: 13.0786, Val Loss: 2018.3148\n",
            "Epoch 193/500, Train Loss: 13.2136, Val Loss: 2018.7958\n",
            "Epoch 194/500, Train Loss: 13.0324, Val Loss: 2017.3927\n",
            "Epoch 195/500, Train Loss: 12.9034, Val Loss: 2015.0464\n",
            "Epoch 196/500, Train Loss: 13.2839, Val Loss: 2008.3083\n",
            "Epoch 197/500, Train Loss: 12.9412, Val Loss: 1998.9081\n",
            "Epoch 198/500, Train Loss: 13.2514, Val Loss: 1996.2328\n",
            "Epoch 199/500, Train Loss: 12.8805, Val Loss: 1997.8052\n",
            "Epoch 200/500, Train Loss: 12.9495, Val Loss: 1991.8279\n",
            "Epoch 201/500, Train Loss: 13.7491, Val Loss: 1987.8257\n",
            "Epoch 202/500, Train Loss: 12.8026, Val Loss: 1979.8948\n",
            "Epoch 203/500, Train Loss: 13.0758, Val Loss: 1975.3719\n",
            "Epoch 204/500, Train Loss: 12.9241, Val Loss: 1965.3494\n",
            "Epoch 205/500, Train Loss: 12.9647, Val Loss: 1960.8038\n",
            "Epoch 206/500, Train Loss: 12.9505, Val Loss: 1961.0319\n",
            "Epoch 207/500, Train Loss: 12.8664, Val Loss: 1958.2736\n",
            "Epoch 208/500, Train Loss: 12.7655, Val Loss: 1957.4470\n",
            "Epoch 209/500, Train Loss: 12.7665, Val Loss: 1949.0168\n",
            "Epoch 210/500, Train Loss: 13.0943, Val Loss: 1945.3104\n",
            "Epoch 211/500, Train Loss: 12.7121, Val Loss: 1946.7465\n",
            "Epoch 212/500, Train Loss: 13.3423, Val Loss: 1936.2732\n",
            "Epoch 213/500, Train Loss: 13.0497, Val Loss: 1933.2306\n",
            "Epoch 214/500, Train Loss: 12.6855, Val Loss: 1938.1694\n",
            "Epoch 215/500, Train Loss: 12.7111, Val Loss: 1929.4669\n",
            "Epoch 216/500, Train Loss: 12.5988, Val Loss: 1926.0262\n",
            "Epoch 217/500, Train Loss: 12.7852, Val Loss: 1922.9763\n",
            "Epoch 218/500, Train Loss: 12.6937, Val Loss: 1916.3473\n",
            "Epoch 219/500, Train Loss: 12.7076, Val Loss: 1914.6934\n",
            "Epoch 220/500, Train Loss: 12.5764, Val Loss: 1912.2239\n",
            "Epoch 221/500, Train Loss: 12.7764, Val Loss: 1911.6868\n",
            "Epoch 222/500, Train Loss: 12.8367, Val Loss: 1903.5836\n",
            "Epoch 223/500, Train Loss: 12.6790, Val Loss: 1896.5089\n",
            "Epoch 224/500, Train Loss: 12.4155, Val Loss: 1895.2084\n",
            "Epoch 225/500, Train Loss: 12.4368, Val Loss: 1891.6117\n",
            "Epoch 226/500, Train Loss: 12.6508, Val Loss: 1886.8324\n",
            "Epoch 227/500, Train Loss: 12.7304, Val Loss: 1877.3188\n",
            "Epoch 228/500, Train Loss: 12.8762, Val Loss: 1870.5917\n",
            "Epoch 229/500, Train Loss: 12.7242, Val Loss: 1873.5667\n",
            "Epoch 230/500, Train Loss: 12.7307, Val Loss: 1874.6305\n",
            "Epoch 231/500, Train Loss: 12.7770, Val Loss: 1874.7401\n",
            "Epoch 232/500, Train Loss: 12.4998, Val Loss: 1859.7170\n",
            "Epoch 233/500, Train Loss: 12.5145, Val Loss: 1862.3868\n",
            "Epoch 234/500, Train Loss: 12.4321, Val Loss: 1860.4342\n",
            "Epoch 235/500, Train Loss: 12.4480, Val Loss: 1854.3011\n",
            "Epoch 236/500, Train Loss: 12.4921, Val Loss: 1849.7839\n",
            "Epoch 237/500, Train Loss: 12.8960, Val Loss: 1843.7860\n",
            "Epoch 238/500, Train Loss: 12.4577, Val Loss: 1843.8221\n",
            "Epoch 239/500, Train Loss: 12.9555, Val Loss: 1843.5040\n",
            "Epoch 240/500, Train Loss: 12.6675, Val Loss: 1834.6046\n",
            "Epoch 241/500, Train Loss: 12.3538, Val Loss: 1823.4818\n",
            "Epoch 242/500, Train Loss: 12.6071, Val Loss: 1825.9751\n",
            "Epoch 243/500, Train Loss: 12.3173, Val Loss: 1815.5432\n",
            "Epoch 244/500, Train Loss: 12.2434, Val Loss: 1822.0504\n",
            "Epoch 245/500, Train Loss: 12.2967, Val Loss: 1812.5745\n",
            "Epoch 246/500, Train Loss: 12.3159, Val Loss: 1799.9940\n",
            "Epoch 247/500, Train Loss: 12.2318, Val Loss: 1802.1368\n",
            "Epoch 248/500, Train Loss: 12.3600, Val Loss: 1789.7472\n",
            "Epoch 249/500, Train Loss: 12.1678, Val Loss: 1801.9894\n",
            "Epoch 250/500, Train Loss: 12.4139, Val Loss: 1789.6312\n",
            "Epoch 251/500, Train Loss: 12.2815, Val Loss: 1791.1349\n",
            "Epoch 252/500, Train Loss: 12.6468, Val Loss: 1782.9165\n",
            "Epoch 253/500, Train Loss: 12.1817, Val Loss: 1782.3939\n",
            "Epoch 254/500, Train Loss: 12.4062, Val Loss: 1773.9359\n",
            "Epoch 255/500, Train Loss: 12.3147, Val Loss: 1766.6483\n",
            "Epoch 256/500, Train Loss: 12.1684, Val Loss: 1759.3390\n",
            "Epoch 257/500, Train Loss: 12.2514, Val Loss: 1758.2074\n",
            "Epoch 258/500, Train Loss: 12.1111, Val Loss: 1754.8029\n",
            "Epoch 259/500, Train Loss: 12.0686, Val Loss: 1749.7372\n",
            "Epoch 260/500, Train Loss: 12.2041, Val Loss: 1748.7040\n",
            "Epoch 261/500, Train Loss: 12.2101, Val Loss: 1735.1385\n",
            "Epoch 262/500, Train Loss: 12.1132, Val Loss: 1731.6149\n",
            "Epoch 263/500, Train Loss: 12.0146, Val Loss: 1729.1478\n",
            "Epoch 264/500, Train Loss: 12.3786, Val Loss: 1735.6357\n",
            "Epoch 265/500, Train Loss: 12.5289, Val Loss: 1723.3253\n",
            "Epoch 266/500, Train Loss: 11.9269, Val Loss: 1710.5061\n",
            "Epoch 267/500, Train Loss: 11.9182, Val Loss: 1719.4432\n",
            "Epoch 268/500, Train Loss: 12.4851, Val Loss: 1710.2437\n",
            "Epoch 269/500, Train Loss: 12.1233, Val Loss: 1707.8386\n",
            "Epoch 270/500, Train Loss: 11.8965, Val Loss: 1693.4403\n",
            "Epoch 271/500, Train Loss: 12.1274, Val Loss: 1688.1260\n",
            "Epoch 272/500, Train Loss: 12.4107, Val Loss: 1692.5507\n",
            "Epoch 273/500, Train Loss: 12.4077, Val Loss: 1681.0750\n",
            "Epoch 274/500, Train Loss: 12.4718, Val Loss: 1679.7366\n",
            "Epoch 275/500, Train Loss: 11.8077, Val Loss: 1672.3912\n",
            "Epoch 276/500, Train Loss: 11.7903, Val Loss: 1662.0110\n",
            "Epoch 277/500, Train Loss: 11.8472, Val Loss: 1663.5841\n",
            "Epoch 278/500, Train Loss: 11.8677, Val Loss: 1655.9933\n",
            "Epoch 279/500, Train Loss: 11.9349, Val Loss: 1653.4689\n",
            "Epoch 280/500, Train Loss: 11.8441, Val Loss: 1642.1798\n",
            "Epoch 281/500, Train Loss: 11.9563, Val Loss: 1640.7081\n",
            "Epoch 282/500, Train Loss: 12.0183, Val Loss: 1634.3341\n",
            "Epoch 283/500, Train Loss: 11.7116, Val Loss: 1634.6288\n",
            "Epoch 284/500, Train Loss: 11.7907, Val Loss: 1625.4465\n",
            "Epoch 285/500, Train Loss: 11.6048, Val Loss: 1621.1030\n",
            "Epoch 286/500, Train Loss: 11.7712, Val Loss: 1610.5576\n",
            "Epoch 287/500, Train Loss: 11.8401, Val Loss: 1614.3792\n",
            "Epoch 288/500, Train Loss: 11.6133, Val Loss: 1602.2207\n",
            "Epoch 289/500, Train Loss: 12.2855, Val Loss: 1601.3134\n",
            "Epoch 290/500, Train Loss: 11.6502, Val Loss: 1591.6483\n",
            "Epoch 291/500, Train Loss: 11.5968, Val Loss: 1592.8132\n",
            "Epoch 292/500, Train Loss: 12.0958, Val Loss: 1593.4143\n",
            "Epoch 293/500, Train Loss: 11.9126, Val Loss: 1580.1544\n",
            "Epoch 294/500, Train Loss: 12.0759, Val Loss: 1576.5159\n",
            "Epoch 295/500, Train Loss: 11.6227, Val Loss: 1575.9027\n",
            "Epoch 296/500, Train Loss: 11.7446, Val Loss: 1572.0320\n",
            "Epoch 297/500, Train Loss: 11.6535, Val Loss: 1543.9933\n",
            "Epoch 298/500, Train Loss: 11.7859, Val Loss: 1549.5699\n",
            "Epoch 299/500, Train Loss: 11.9408, Val Loss: 1557.5040\n",
            "Epoch 300/500, Train Loss: 12.0575, Val Loss: 1545.8584\n",
            "Epoch 301/500, Train Loss: 11.4552, Val Loss: 1550.1034\n",
            "Epoch 302/500, Train Loss: 12.2391, Val Loss: 1546.4974\n",
            "Early stopping after 302 epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▇▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▆▅▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>302</td></tr><tr><td>train_loss</td><td>12.23906</td></tr><tr><td>val_loss</td><td>1546.49744</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">exp-001</strong> at: <a href='https://wandb.ai/abrar39/time-series-forecasting/runs/gid0gz4s' target=\"_blank\">https://wandb.ai/abrar39/time-series-forecasting/runs/gid0gz4s</a><br/> View project at: <a href='https://wandb.ai/abrar39/time-series-forecasting' target=\"_blank\">https://wandb.ai/abrar39/time-series-forecasting</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241215_151943-gid0gz4s/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the Model\n"
      ],
      "metadata": {
        "id": "wOYifgKFWoLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create the DataLoader for validation data\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize lists to store actual and predicted values\n",
        "actuals = []\n",
        "predictions = []\n",
        "\n",
        "# Loop through the validation set\n",
        "with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for inputs, targets in val_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Store actual and predicted values\n",
        "        actuals.append(targets.numpy())\n",
        "        predictions.append(outputs.numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "actuals = np.concatenate(actuals, axis=0)\n",
        "predictions = np.concatenate(predictions, axis=0)"
      ],
      "metadata": {
        "id": "jWNPI3hhRQPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs predicted values for validation set\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actuals, label=\"Actual Values\", color='blue', linestyle='-', marker='o')\n",
        "plt.plot(predictions, label=\"Predicted Values\", color='red', linestyle='--', marker='x')\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Close Value\")\n",
        "plt.title(\"Actual vs Predicted Close Values (Validation Set)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z0VECJ-mWr0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs predicted values for the first few samples\n",
        "num_samples_to_display = 10  # Set how many samples you want to display\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the actual values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(actuals[:num_samples_to_display], color='blue', marker='o', label=\"Actual Values\")\n",
        "plt.title(\"Actual Values\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Close Value\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the predicted values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(predictions[:num_samples_to_display], color='red', marker='x', label=\"Predicted Values\")\n",
        "plt.title(\"Predicted Values\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Close Value\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H41tX4NvWwfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22JuFi3tW-OU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}